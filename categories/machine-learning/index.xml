<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on </title>
    <link>https://blog.x-fei.me/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 03 Mar 2019 19:42:33 +0800</lastBuildDate>
    
	<atom:link href="https://blog.x-fei.me/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Convolution&#39;s Numpy and Pytorch implementation</title>
      <link>https://blog.x-fei.me/posts/ml/convolution_in_pytorch/</link>
      <pubDate>Sun, 03 Mar 2019 19:42:33 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/ml/convolution_in_pytorch/</guid>
      <description>&lt;p&gt;初学CNN的时候，比较疑惑输入的维度是&lt;code&gt;(BatchSize, Channels, Height, Width)&lt;/code&gt;的&lt;code&gt;feature map&lt;/code&gt;经过size是k的卷积核后变成了输出是什么？以及它是怎么实现的？
本文主要讲解并探究卷积实现。&lt;br /&gt;
首先简单聊一下卷积操作，介绍&lt;code&gt;Pytorch&lt;/code&gt;中卷积的&lt;code&gt;API&lt;/code&gt;；&lt;br /&gt;
接着我们自己用&lt;code&gt;python&lt;/code&gt;实现简单卷积，并与&lt;code&gt;API&lt;/code&gt;调用结果进行对比；&lt;br /&gt;
之后我们进一步去了解&lt;code&gt;Pytorch&lt;/code&gt;中卷积的实现源码。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SVM(Part II)</title>
      <link>https://blog.x-fei.me/posts/svm2/</link>
      <pubDate>Wed, 26 Dec 2018 00:47:04 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/svm2/</guid>
      <description>&lt;p&gt;About These  Articles：&lt;/p&gt;

&lt;p&gt;SVM的学习，可以查询到大量的相关文章、视频，并且在几本经典的书（西瓜书，统计学习方法等）中也有相应的解读。本文为个人总结回顾。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SVM(Part I)</title>
      <link>https://blog.x-fei.me/posts/svm/</link>
      <pubDate>Sun, 23 Dec 2018 00:21:34 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/svm/</guid>
      <description>&lt;p&gt;About These  Articles：&lt;/p&gt;

&lt;p&gt;SVM的学习，可以查询到大量的相关文章、视频，并且在几本经典的书（西瓜书，统计学习方法等）中也有相应的解读。本文为个人总结回顾。(由于Markdown和MathJax的兼容问题，很多公式没有渲染出来，可以直接看手写推导过程。)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>8-puzzles algorithm</title>
      <link>https://blog.x-fei.me/posts/8-puzzles-algorithm/</link>
      <pubDate>Sat, 20 Oct 2018 09:09:29 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/8-puzzles-algorithm/</guid>
      <description>&lt;p&gt;&lt;center&gt;AIMA: 8-puzzles&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;题目描述&#34;&gt;题目描述&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;8-puzzle problem&lt;/strong&gt;&lt;br /&gt;
Given any randomly generated start state and a goal state shown below, implement the IDS, greedy search and A* search algorithms, respectively, to find a sequence of actions that will transform the state from the start state to the goal state.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>axis and argmax function</title>
      <link>https://blog.x-fei.me/posts/argmax-function-and-its-usage/</link>
      <pubDate>Tue, 16 Oct 2018 00:53:13 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/argmax-function-and-its-usage/</guid>
      <description>&lt;p&gt;简单讨论一下&lt;code&gt;argmax&lt;/code&gt;函数及其用法，由于其在&lt;code&gt;numpy&lt;/code&gt;和&lt;code&gt;PyTorch&lt;/code&gt;中都有出现，所以先在&lt;code&gt;numpy&lt;/code&gt;中讨论，然后补充介绍在&lt;code&gt;PyTorch&lt;/code&gt;中的用法。&lt;/p&gt;

&lt;p&gt;同理我们可以理解&lt;code&gt;argmin&lt;/code&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GD &amp; SGD 理解与二维模拟</title>
      <link>https://blog.x-fei.me/posts/gd-sgd-%E7%90%86%E8%A7%A3%E4%B8%8E%E4%BA%8C%E7%BB%B4%E6%A8%A1%E6%8B%9F/</link>
      <pubDate>Wed, 10 Oct 2018 21:12:56 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/gd-sgd-%E7%90%86%E8%A7%A3%E4%B8%8E%E4%BA%8C%E7%BB%B4%E6%A8%A1%E6%8B%9F/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;center&gt;Machine Learning Exercise 2&lt;/center&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验题目&#34;&gt;实验题目&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Generate n = 2,000 points uniformly at random in the two-dimensional unit square. Which point do you expect the centroid to be?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;What objective does the centroid of the points optimize?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Apply gradient descent (GD) to find the centroid.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Apply stochastic gradient descent (SGD) to find the centroid. Can you
say in simple words, what the algorithm is doing?&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>多项式拟合-线性回归</title>
      <link>https://blog.x-fei.me/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88/</link>
      <pubDate>Mon, 24 Sep 2018 23:24:56 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;center&gt;Machine Learning Exercise 1&lt;/center&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验题目&#34;&gt;实验题目&lt;/h3&gt;

&lt;p&gt;编写程序:模拟仿真多项式回归
参见 textbook p4-12(PRML)。完成以下任务:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PyTorch Api Notes 📙</title>
      <link>https://blog.x-fei.me/posts/pytorch-notes/</link>
      <pubDate>Thu, 13 Sep 2018 01:14:38 +0800</pubDate>
      
      <guid>https://blog.x-fei.me/posts/pytorch-notes/</guid>
      <description>&lt;p&gt;&lt;center&gt;Simple Api Notes ✍🏼 For Beginners！&lt;/center&gt;&lt;br /&gt;
记录并尝试解释一些常见的Api，并部分介绍它们的原理、实战运用。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>